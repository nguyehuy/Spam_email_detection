# -*- coding: utf-8 -*-
"""processEmail.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l1k_udOb75GphQF4Iq765UVucKLwYaFh
"""

# This class is used for prefrocessing data

import re
class trasformData(object):
    import re
    def __init__(self):
        pass
    # Every email has a header, we need to remove the header
    def remove_header(self,content):
        i=content.find('\n\n')
        content=content[i+2:]
        return content
    # The email is conveted into lower case, so thar the captialization is ignored
    def lower(self,content):
        return content.lower()
    
    # Removing theHTML tags in email
    def stripping_html(self,content):
        TAG_RE= re.compile(r'<[^>]+>')
        return TAG_RE.sub('', content)
    
    
    # All URLs are replace with the text 'httpaddr'
    def normalizing(self,content):
        content=re.sub(r'(http|https)://[^\s]*',r'httpaddr', content)
        content=re.sub(r'[^\s]+@[^\s]+',r'emailaddr', content)
        content=re.sub(r'[0-9]+',r'number', content)
        content=re.sub(r'[$]+',r'dollar', content)
        return content
    
    
    # Reduce the word to their stemmed form
    # For example, "discount", "discounts", "discounted" and "discounting" 
    # are all replaced with \discount"
    def stemming_word(self,content):
        from nltk.stem import PorterStemmer
        single=[]
        stemmer = PorterStemmer()
        for plural in content.split():
            single.append(stemmer.stem(plural))
        content=' '.join(single)
        return content
    
    # Remove all non-words (punctuation). All white spaces (tabs, spaces, newlines) have been 
    # trimmed to a single space character
    def strip_punctuation(self,content):
        from string import punctuation
        import string
        translator=str.maketrans(string.punctuation, ' '*len(string.punctuation))
        content=content.translate(translator)
        content=re.sub('\s+',' ',content)
        return content
    
    #Remove all the stopwords ('a', 'an', 'is', ..) becasue it is not neccesary 
    def remove_stopwords(self, content):
        import nltk
        nltk.download('stopwords')
        nltk.download('punkt')
        from nltk.corpus import stopwords
        stop_words = set(stopwords.words('english'))
        from nltk.tokenize import word_tokenize
        tokens = word_tokenize(content)
        content = ' '.join(i for i in tokens if not i in stop_words)
        return content
    # Call all the above functions
    def transform(self, content):
        content=self.remove_header(content)
        content=self.lower(content)
        content=self.stripping_html(content)
        content=self.normalizing(content)
        content=self.stemming_word(content)
        content=self.remove_stopwords(content)
        content=self.strip_punctuation(content)
        return content
    